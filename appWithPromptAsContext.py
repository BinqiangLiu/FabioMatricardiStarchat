#Memory in prompt.å°†è¿‡å¾€çš„å¯¹è¯å†å²æ–‡æœ¬å†…å®¹æå–å‡ºæ¥å¹¶è¿½åŠ ç´¯ç§¯ï¼Œç„¶åæ”¾å…¥mypromptä¸­ä½œä¸ºç”¨æˆ·è¾“å…¥çš„é—®é¢˜ï¼ˆè¿˜æœ‰ä¸€äº›å°çš„å¤„ç†ï¼Œå¢åŠ ä¸€äº›å›ºå®šçš„æ–‡å­—è¯´æ˜ï¼‰
#/home/adminuser/venv/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:127: FutureWarning: '__init__' (from 'huggingface_hub.inference_api')
#is deprecated and will be removed from version '0.19.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`.
#Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.
from pathlib import Path
import streamlit as st
from streamlit_chat import message
from huggingface_hub import InferenceClient
from langchain import HuggingFaceHub
import requests# Internal usage
import os
from dotenv import load_dotenv
from time import sleep
#from hugchat import hugchat
#from hugchat.login import Login
from streamlit_extras.colored_header import colored_header
from streamlit_extras.add_vertical_space import add_vertical_space

st.set_page_config(page_title="AI Chatbot 100% Free", layout="wide")
st.write('å®Œå…¨å¼€æºå…è´¹çš„AIæ™ºèƒ½èŠå¤©åŠ©æ‰‹ | Absolute Free & Opensouce AI Chatbot')

# --- PATH SETTINGS ---
css_file = "main.css"
# --- LOAD CSS, PDF & PROFIL PIC ---
with open(css_file) as f:
    st.markdown("<style>{}</style>".format(f.read()), unsafe_allow_html=True)

load_dotenv()
yourHFtoken = "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"
yourHFtoken = os.getenv("HUGGINGFACEHUB_API_TOKEN")
repo_id="HuggingFaceH4/starchat-beta"
myprompt_temp=""
myprompt=""
#AVATARS
#av_us = './man.png' #"ğŸ¦–" #A single emoji, e.g. "ğŸ§‘ ğŸ’»", "ğŸ¤–", "ğŸ¦–". Shortco
#av_ass = './robot.png'
av_us = 'ğŸ§‘'
av_ass = 'ğŸ¤–'
# Set a default model
if "hf_model" not in st.session_state:
    st.session_state["hf_model"] = "HuggingFaceH4/starchat-beta"

### INITIALIZING STARCHAT FUNCTION MODEL
def starchat(model,myprompt, your_template):
    from langchain import PromptTemplate, LLMChain
    os.environ["HUGGINGFACEHUB_API_TOKEN"] = yourHFtoken
    llm = HuggingFaceHub(repo_id=repo_id,
                         model_kwargs={"min_length":100,
                                       "max_new_tokens":1024, "do_sample":True,
                                       "temperature":0.1,
                                       "top_k":50,
                                       "top_p":0.95, "eos_token_id":49155})
#ä»¥ä¸‹æ˜¯æ–°å¢å†…å®¹
#    my_prompt_template = """You are a very smart and helpful AI assistant. You are provided {contexts} as chat history between the user and you.
#    For any following question, you MUST consider the chat history and response to {myprompt} as the user question. 
#    However, you SHOULD NOT limit your reponse to the chat history.
#    You should take any actions you would take when you response to a user question normally.
#    DO NOT OUTPUT the chat history or the user question or ANY other unrelated information!
#    AI Response:
#    """
#ä»¥ä¸Šæ˜¯æ–°å¢å†…å®¹    
#    template = my_prompt_template
    template = your_template
#    prompt = PromptTemplate(template=template, input_variables=["contexts", "myprompt"])
    prompt = PromptTemplate(template=template, input_variables=["myprompt"])
    llm_chain = LLMChain(prompt=prompt, llm=llm)
    add_notes_1="Beginning of chat history:\n"
    add_notes_2="End of chat history.\n"
    add_notes_3="Please consult the above chat history before responding to the user question below.\n"
    add_notes_4="User question: "
    myprompt_temp=myprompt
    #myprompt=add_notes_1+contexts+add_notes_2+add_notes_3+myprompt
    myprompt = add_notes_1 + "\n" + contexts + "\n" + add_notes_2 + "\n" + add_notes_3 + "\n"+ add_notes_4 + "\n" + myprompt
#ä¼¼ä¹èƒ½å¤Ÿè¿è¡Œä¸å‡ºé”™ï¼Œä½†æ˜¯è¿è¡Œé€Ÿåº¦å¾ˆæ…¢ï¼Ÿï¼æ›´é‡è¦çš„æ˜¯ï¼Œå¥½åƒè¿˜æ˜¯ä¸èƒ½å¤Ÿå°†ä»¥å‰çš„å¯¹è¯å†å²çº³å…¥ï¼
    #st.write("---åœ¨def starchat(model,myprompt, your_template)å†…çš„ä¿¡æ¯æ‰“å°è¾“å‡ºå¼€å§‹")
    #st.write("Current User Query: "+myprompt_temp)
    #st.write("---")
    #st.write("Combined User Input as Prompt:")
    #st.write(myprompt)
    #st.write("---åœ¨def starchat(model,myprompt, your_template)å†…çš„ä¿¡æ¯æ‰“å°è¾“å‡ºç»“æŸ")
    llm_reply = llm_chain.run(myprompt)
    #llm_reply = llm_chain.run({'contexts': contexts, 'myprompt': myprompt})    
    reply = llm_reply.partition('<|end|>')[0]
    return reply

# FUNCTION TO LOG ALL CHAT MESSAGES INTO chathistory.txt
#def writehistory(text):
#    with open('chathistory.txt', 'a') as f:
#        f.write(text)
#        f.write('\n')
#å¢åŠ ä¸‹é¢ä¸€è¡Œä»£ç ï¼Œè¯»å–å¯¹è¯è®°å½•textå¹¶å­˜å‚¨åˆ°contexts
#        contexts = f.read()
#    f.close()

def writehistory(text):
    with open('chathistory.txt', 'a+') as f:
        f.write(text)
        f.write('\n')
        f.seek(0)  # å°†æ–‡ä»¶æŒ‡é’ˆç§»åŠ¨åˆ°æ–‡ä»¶å¼€å¤´
        contexts = f.read()
    return contexts

### START STREAMLIT UI
#st.title("ğŸ¤— HuggingFace Free ChatBot")
#st.subheader("using Starchat-beta")

# Initialize chat history
if "messages" not in st.session_state:
   st.session_state.messages = []
# Display chat messages from history on app rerun
for message in st.session_state.messages:
   if message["role"] == "user":
#      with st.chat_message(message["role"],avatar=av_us):
      with st.chat_message(message["role"]):
           st.write("ç”¨æˆ·è¾“å…¥é—®é¢˜çš„æ˜¾ç¤ºå¼€å§‹")
           st.markdown(message["content"])
           st.write("ç”¨æˆ·è¾“å…¥é—®é¢˜çš„æ˜¾ç¤ºç»“æŸ")
   else:
#       with st.chat_message(message["role"],avatar=av_ass):
       with st.chat_message(message["role"]):
           st.write("assistantå›å¤å†…å®¹çš„æ˜¾ç¤ºå¼€å§‹")
           st.markdown(message["content"])
           st.write("assistantå›å¤å†…å®¹çš„æ˜¾ç¤ºç»“æŸ")

# Accept user input
if myprompt := st.chat_input("Enter your question here."):
    # Add user message to chat history
    st.session_state.messages.append({"role": "user", "content": myprompt})
    # Display user message in chat message container
#    with st.chat_message("user", avatar=av_us):
    with st.chat_message("user"):
        st.markdown(myprompt)
        usertext = f"user: {myprompt}"
#        writehistory(usertext)
#æ–°å¢å¦‚ä¸‹ä¸€è¡Œ        
        contexts = writehistory(usertext)
        st.write("st.chat_messageçš„userä¹‹contexts: "+contexts)
        # Display assistant response in chat message container
    with st.chat_message("assistant"):
        with st.spinner("AI Thinking..."):
            message_placeholder = st.empty()
            full_response = ""
            st.write("---assistantçš„å›å¤ç»“æœè¾“å‡ºå¼€å§‹---")
            res = starchat(
                  st.session_state["hf_model"],
                  myprompt, "<|system|>\n<|end|>\n<|user|>\n{myprompt}<|end|>\n<|assistant|>")
            response = res.split(" ")
            st.write("---assistantçš„å›å¤ç»“æœè¾“å‡ºç»“æŸ---")
            for r in response:
                full_response = full_response + r + " "
                message_placeholder.markdown(full_response + "â–Œ")
                sleep(0.1)            
            st.write("ç”¨st.writeæ–¹æ³•æ‰“å°è¾“å‡ºassistantçš„å›å¤ç»“æœå¼€å§‹")
            st.write("assistantçš„å›å¤ç»“æœ: "+full_response)
            st.write("ç”¨st.writeæ–¹æ³•æ‰“å°è¾“å‡ºassistantçš„å›å¤ç»“æœç»“æŸ")
            st.write("---")
            #message_placeholder.markdown(full_response)   è¿™ä¸ªæ˜¯ä¸æ˜¯ç”¨æ¥æ˜¾ç¤ºassistantçš„æ–¹æ³•ï¼Ÿï¼Ÿï¼Ÿ
            #st.write("---åœ¨with st.chat_message( - assistant - )å†…çš„ä¿¡æ¯æ‰“å°è¾“å‡ºå¼€å§‹")
            #st.write("Current User Query: "+myprompt_temp)
            #st.write("---")
            #st.write("Combined User Input as Prompt:")
            #st.write(myprompt)
            #åœ¨è¿™é‡Œï¼Œå˜é‡myprompt_tempã€mypromptéƒ½ä¼šè¢«é‡ç½®ä¸ºç©ºç½®
            #st.write("---åœ¨with st.chat_message( - assistant - )å†…çš„ä¿¡æ¯æ‰“å°è¾“å‡ºç»“æŸ") - ä½ç½®ä¸å¯¹
            asstext = f"assistant: {full_response}"            
#            writehistory(asstext)
#æ–°å¢å¦‚ä¸‹ä¸€è¡Œ        
            contexts = writehistory(asstext)
            st.write("st.chat_messageçš„assistantä¹‹contexts: "+contexts)
            st.session_state.messages.append({"role": "assistant", "content": full_response})
